from spikegadgets_to_nwb.data_scanner import get_file_info
from spikegadgets_to_nwb.convert import create_nwbs, _create_nwb

import os
import pandas as pd
import numpy as np
from pathlib import Path
from pynwb import NWBHDF5IO

path = os.path.dirname(os.path.abspath(__file__))

MICROVOLTS_PER_VOLT = 1e6


def test_get_file_info():
    try:
        # running on github
        data_path = Path(os.environ.get("DOWNLOAD_DIR"))
    except (TypeError, FileNotFoundError):
        # running locally
        data_path = Path(path)
    path_df = get_file_info(data_path)

    for file_type in [
        ".h264",
        ".stateScriptLog",
        ".cameraHWSync",
        ".videoTimeStamps",
        ".videoPositionTracking",
        ".rec",
        ".trackgeometry",
        ".stateScriptLog",
    ]:
        assert len(path_df[path_df.file_extension == file_type]) == 2

    assert set(path_df.animal) == {"sample"}
    assert set(path_df.date) == {20230622}
    assert set(path_df.epoch) == {1, 2}
    assert (set(path_df.tag) == {"a1"}) or (
        set(path_df.tag) == {"a1", "NA"}
    )  # yamlfiles only added in local testing
    for file in path_df.full_path:
        assert Path(file).exists()


def test_convert():
    try:
        # running on github
        data_path = Path(os.environ.get("DOWNLOAD_DIR"))
        yml_data_path = Path(path + "/test_data")
        yml_path_df = get_file_info(yml_data_path)
        yml_path_df = yml_path_df[yml_path_df.file_extension == ".yml"]
        append_yml_df = True
    except (TypeError, FileNotFoundError):
        # running locally
        data_path = Path(path + "/test_data")
        append_yml_df = False
    probe_metadata = [Path(path + "/test_data/tetrode_12.5.yml")]

    # make session_df
    path_df = get_file_info(data_path)
    if append_yml_df:
        path_df = path_df[
            path_df.file_extension != ".yml"
        ]  # strip ymls, fixes github runner issue where yamls only sometimes present between jobs
        path_df = pd.concat([path_df, yml_path_df])
        path_df = path_df[
            path_df.full_path
            != yml_data_path.as_posix() + "/20230622_sample_metadataProbeReconfig.yml"
        ]
    else:
        path_df = path_df[
            path_df.full_path
            != data_path.as_posix() + "/20230622_sample_metadataProbeReconfig.yml"
        ]
    session_df = path_df[(path_df.animal == "sample")]
    assert len(session_df[session_df.file_extension == ".yml"]) == 1
    _create_nwb(
        session=("20230622", "sample", "1"),
        session_df=session_df,
        probe_metadata_paths=probe_metadata,
        output_dir=str(data_path),
    )
    assert "sample20230622.nwb" in os.listdir(str(data_path))
    with NWBHDF5IO(str(data_path) + "/sample20230622.nwb") as io:
        nwbfile = io.read()
        with NWBHDF5IO(str(data_path) + "/minirec20230622_.nwb") as io2:
            old_nwbfile = io2.read()

            # run nwb comparison
            compare_nwbfiles(nwbfile, old_nwbfile)
    # cleanup
    os.remove(str(data_path) + "/sample20230622.nwb")


def check_module_entries(test, reference):
    todo = [
        "camera_sample_frame_counts",
        "video_files",
        "dataacq_device0",
    ]  # TODO: known missing entries
    for entry in reference:
        if entry in todo:
            continue
        assert entry in test


def compare_nwbfiles(nwbfile, old_nwbfile, truncated_size=False):
    """Compare two nwbfiles, checking that all the same entries are present and that the data matches

    Parameters
    ----------
    nwbfile : pynwb.NWBFile
        The nwbfile to be tested
    old_nwbfile : pynwb.NWBFile
        The reference nwbfile (generated by rec_to_nwb)
    truncated_size : bool, optional
        Whether the new nwbfile only contains a subset of the data, by default False
    """

    # check existence of contents
    check_module_entries(nwbfile.processing, old_nwbfile.processing)
    check_module_entries(nwbfile.acquisition, old_nwbfile.acquisition)
    check_module_entries(nwbfile.devices, old_nwbfile.devices)
    assert nwbfile.subject
    assert nwbfile.session_description
    assert nwbfile.session_id
    assert nwbfile.session_start_time
    assert nwbfile.electrodes
    assert nwbfile.experiment_description
    assert nwbfile.experimenter
    assert nwbfile.file_create_date
    assert nwbfile.identifier
    assert nwbfile.institution
    assert nwbfile.lab

    # check ephys data values
    conversion = nwbfile.acquisition["e-series"].conversion * MICROVOLTS_PER_VOLT
    assert (
        (nwbfile.acquisition["e-series"].data[0, :] * conversion).astype("int16")
        == old_nwbfile.acquisition["e-series"].data[0, :]
    ).all()
    # check data shapes match if untruncated
    assert (
        nwbfile.acquisition["e-series"].data.shape
        == old_nwbfile.acquisition["e-series"].data.shape
    ) or truncated_size
    ephys_size = nwbfile.acquisition["e-series"].data.shape[0]
    # check all values of one of the streams
    assert (
        (nwbfile.acquisition["e-series"].data[:, 0] * conversion).astype("int16")
        == old_nwbfile.acquisition["e-series"].data[:ephys_size, 0]
    ).all()
    # check that timestamps are less than one sample different
    assert np.isclose(
        nwbfile.acquisition["e-series"].timestamps[:],
        old_nwbfile.acquisition["e-series"].timestamps[:ephys_size],
        rtol=0,
        atol=1.0 / 30000,
    ).all()

    # check analog data
    # get index mapping of channels
    id_order = nwbfile.processing["analog"]["analog"]["analog"].description.split(
        "   "
    )[:-1]
    old_id_order = old_nwbfile.processing["analog"]["analog"][
        "analog"
    ].description.split("   ")[:-1]
    index_order = [old_id_order.index(id) for id in id_order]
    # TODO check that all the same channels are present

    # compare analog data
    assert (
        nwbfile.processing["analog"]["analog"]["analog"].data.shape[0]
        == old_nwbfile.processing["analog"]["analog"]["analog"].data.shape[0]
    )
    assert (
        nwbfile.processing["analog"]["analog"]["analog"].data.shape[0]
        == old_nwbfile.processing["analog"]["analog"]["analog"].data.shape[0]
    ) or truncated_size
    analog_size = nwbfile.processing["analog"]["analog"]["analog"].data.shape[0]
    # compare matching for first timepoint
    assert (
        nwbfile.processing["analog"]["analog"]["analog"].data[0, :]
        == old_nwbfile.processing["analog"]["analog"]["analog"].data[0, :][index_order]
    ).all()
    # compare one channel across all timepoints
    assert (
        nwbfile.processing["analog"]["analog"]["analog"].data[:, 0]
        == old_nwbfile.processing["analog"]["analog"]["analog"].data[
            :analog_size, index_order[0]
        ]
    ).all()

    # compare dio data
    for old_dio in old_nwbfile.processing["behavior"][
        "behavioral_events"
    ].time_series.values():
        current_dio = nwbfile.processing["behavior"]["behavioral_events"][old_dio.name]
        # check that timeseries match
        dio_size = current_dio.data.shape[0]
        np.testing.assert_array_equal(current_dio.data[:], old_dio.data[:dio_size])
        assert np.allclose(
            current_dio.timestamps[:],
            old_dio.timestamps[:dio_size],
            rtol=0,
            atol=1.0 / 30000,
        )
        assert current_dio.unit == old_dio.unit
        assert current_dio.description == old_dio.description
