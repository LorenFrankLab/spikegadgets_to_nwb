import logging
import os
import re
import subprocess
from pathlib import Path
from xml.etree import ElementTree

import numpy as np
import pandas as pd
from pynwb import NWBFile, TimeSeries
from pynwb.behavior import BehavioralEvents, Position
from pynwb.image import ImageSeries
from scipy.ndimage import label
from scipy.stats import linregress

from spikegadgets_to_nwb.convert_rec_header import detect_ptp_from_header

NANOSECONDS_PER_SECOND = 1e9


def parse_dtype(fieldstr: str) -> np.dtype:
    """
    Parses the last fields parameter (<time uint32><...>) as a single string.
    Assumes it is formatted as <name number * type> or <name type>. Returns a numpy dtype object.

    Parameters
    ----------
    fieldstr : str
        The string to parse.

    Returns
    -------
    np.dtype
        The numpy dtype object.

    Raises
    ------
    AttributeError
        If the field type is not valid.

    Examples
    --------
    >>> fieldstr = '<time uint32><x float32><y float32><z float32>'
    >>> parse_dtype(fieldstr)
    dtype([('time', '<u4'), ('x', '<f4'), ('y', '<f4'), ('z', '<f4')])

    """
    # Returns np.dtype from field string
    sep = " ".join(
        fieldstr.replace("><", " ").replace(">", " ").replace("<", " ").split()
    ).split()

    typearr = []

    # Every two elemets is fieldname followed by datatype
    for i in range(0, len(sep), 2):
        fieldname = sep[i]
        repeats = 1
        ftype = "uint32"
        # Finds if a <num>* is included in datatype
        if "*" in sep[i + 1]:
            temptypes = re.split("\*", sep[i + 1])
            # Results in the correct assignment, whether str is num*dtype or dtype*num
            ftype = temptypes[temptypes[0].isdigit()]
            repeats = int(temptypes[temptypes[1].isdigit()])
        else:
            ftype = sep[i + 1]
        try:
            fieldtype = getattr(np, ftype)
        except AttributeError:
            raise AttributeError(ftype + " is not a valid field type.\n")
        else:
            typearr.append((str(fieldname), fieldtype, repeats))

    return np.dtype(typearr)


def read_trodes_datafile(filename: Path) -> dict:
    """
    Read trodes binary.

    Parameters
    ----------
    filename : Path
        Path to the trodes binary file.

    Returns
    -------
    dict
        A dictionary containing the settings and data from the trodes binary file.

    Raises
    ------
    Exception
        If the settings format is not supported.

    """
    with open(filename, "rb") as file:
        # Check if first line is start of settings block
        if file.readline().decode().strip() != "<Start settings>":
            raise Exception("Settings format not supported")
        fields_text = dict()
        for line in file:
            # Read through block of settings
            line = line.decode().strip()
            # filling in fields dict
            if line != "<End settings>":
                settings_name, setting = line.split(": ")
                fields_text[settings_name.lower()] = setting
            # End of settings block, signal end of fields
            else:
                break
        # Reads rest of file at once, using dtype format generated by parse_dtype()
        try:
            fields_text["data"] = np.fromfile(
                file, dtype=parse_dtype(fields_text["fields"])
            )
        except KeyError:
            fields_text["data"] = np.fromfile(file)
        return fields_text


def get_framerate(timestamps: np.ndarray) -> float:
    """
    Calculates the framerate of a video based on the timestamps of each frame.

    Parameters
    ----------
    timestamps : np.ndarray
        An array of timestamps for each frame in the video.

    Returns
    -------
    frame_rate: float
        The framerate of the video in frames per second.
    """
    timestamps = np.asarray(timestamps)
    return NANOSECONDS_PER_SECOND / np.median(np.diff(timestamps))


def find_acquisition_timing_pause(
    timestamps: np.ndarray,
    min_duration: float = 0.4,
    max_duration: float = 1.0,
    n_search: int = 100,
) -> float:
    """
    Find the midpoint time of a timing pause in the video stream.

    Parameters
    ----------
    timestamps : np.ndarray
        An array of timestamps for each frame in the video.
    min_duration : float, optional
        The minimum duration of the pause in seconds, by default 0.4.
    max_duration : float, optional
        The maximum duration of the pause in seconds, by default 1.0.
    n_search : int, optional
        The number of frames to search for the pause, by default 100.

    Returns
    -------
    pause_mid_time : float
        The midpoint time of the timing pause.

    """
    timestamps = np.asarray(timestamps)
    timestamp_difference = np.diff(timestamps[:n_search] / NANOSECONDS_PER_SECOND)
    is_valid_gap = (timestamp_difference > min_duration) & (
        timestamp_difference < max_duration
    )
    pause_start_ind = np.nonzero(is_valid_gap)[0][0]
    pause_end_ind = pause_start_ind + 1
    pause_mid_time = (
        timestamps[pause_start_ind]
        + (timestamps[pause_end_ind] - timestamps[pause_start_ind]) // 2
    )

    return pause_mid_time


def find_large_frame_jumps(
    frame_count: np.ndarray, min_frame_jump: int = 15
) -> np.ndarray:
    """
    Find large frame jumps in the video.

    Parameters
    ----------
    frame_count : np.ndarray
        An array of frame counts for each frame in the video.
    min_frame_jump : int, optional
        The minimum number of frames to consider a jump as large, by default 15.

    Returns
    -------
    np.ndarray
        A boolean array indicating whether each frame has a large jump.

    """
    logger = logging.getLogger("convert")
    frame_count = np.asarray(frame_count)

    is_large_frame_jump = np.insert(np.diff(frame_count) > min_frame_jump, 0, False)

    logger.info(f"big frame jumps: {np.nonzero(is_large_frame_jump)[0]}")

    return is_large_frame_jump


def detect_repeat_timestamps(timestamps: np.ndarray) -> np.ndarray:
    """
    Detects repeated timestamps in an array of timestamps.

    Parameters
    ----------
    timestamps : np.ndarray
        Array of timestamps.

    Returns
    -------
    np.ndarray
        Boolean array indicating whether each timestamp is repeated.
    """
    return np.insert(timestamps[:-1] >= timestamps[1:], 0, False)


import numpy as np


def detect_trodes_time_repeats_or_frame_jumps(
    trodes_time: np.ndarray, frame_count: np.ndarray
) -> tuple[np.ndarray, np.ndarray]:
    """
    Detects if a Trodes time index repeats, indicating that the Trodes clock has frozen
    due to headstage disconnects. Also detects large frame jumps.

    Parameters
    ----------
    trodes_time : np.ndarray
        Array of Trodes time indices.
    frame_count : np.ndarray
        Array of frame counts.

    Returns
    -------
    tuple[np.ndarray, np.ndarray]
        A tuple containing two arrays:
        - non_repeat_timestamp_labels : np.ndarray
            Array of labels for non-repeating timestamps.
        - non_repeat_timestamp_labels_id : np.ndarray
            Array of unique IDs for non-repeating timestamps.
    """
    logger = logging.getLogger("convert")

    trodes_time = np.asarray(trodes_time)
    is_repeat_timestamp = detect_repeat_timestamps(trodes_time)
    logger.info(f"repeat timestamps ind: {np.nonzero(is_repeat_timestamp)[0]}")

    is_large_frame_jump = find_large_frame_jumps(frame_count)
    is_repeat_timestamp = np.logical_or(is_repeat_timestamp, is_large_frame_jump)

    repeat_timestamp_labels = label(is_repeat_timestamp)[0]
    repeat_timestamp_labels_id, repeat_timestamp_label_counts = np.unique(
        repeat_timestamp_labels, return_counts=True
    )
    is_repeat = np.logical_and(
        repeat_timestamp_labels_id != 0, repeat_timestamp_label_counts > 2
    )
    repeat_timestamp_labels_id = repeat_timestamp_labels_id[is_repeat]
    repeat_timestamp_label_counts = repeat_timestamp_label_counts[is_repeat]
    is_repeat_timestamp[
        ~np.isin(repeat_timestamp_labels, repeat_timestamp_labels_id)
    ] = False

    non_repeat_timestamp_labels = label(~is_repeat_timestamp)[0]
    non_repeat_timestamp_labels_id = np.unique(non_repeat_timestamp_labels)
    non_repeat_timestamp_labels_id = non_repeat_timestamp_labels_id[
        non_repeat_timestamp_labels_id != 0
    ]

    return (non_repeat_timestamp_labels, non_repeat_timestamp_labels_id)


def estimate_camera_time_from_mcu_time(
    position_timestamps: np.ndarray, mcu_timestamps: np.ndarray
) -> tuple[np.ndarray, np.ndarray]:
    """

    Parameters
    ----------
    position_timestamps : pd.DataFrame
    mcu_timestamps : pd.DataFrame

    Returns
    -------
    camera_systime : np.ndarray, shape (n_frames_within_neural_time,)
    is_valid_camera_time : np.ndarray, shape (n_frames,)

    """
    is_valid_camera_time = np.isin(position_timestamps.index, mcu_timestamps.index)
    camera_systime = np.asarray(
        mcu_timestamps.loc[position_timestamps.index[is_valid_camera_time]]
    )

    return camera_systime, is_valid_camera_time


def estimate_camera_to_mcu_lag(
    camera_systime: np.ndarray, dio_systime: np.ndarray, n_breaks: int = 0
) -> float:
    logger = logging.getLogger("convert")
    if n_breaks == 0:
        dio_systime = dio_systime[: len(camera_systime)]
        camera_to_mcu_lag = np.median(camera_systime - dio_systime)
    else:
        camera_to_mcu_lag = camera_systime[0] - dio_systime[0]

    logger.info(
        "estimated trodes to camera lag: "
        f"{camera_to_mcu_lag / NANOSECONDS_PER_SECOND:0.3f} s"
    )
    return camera_to_mcu_lag


def remove_acquisition_timing_pause_non_ptp(
    dio_systime: np.ndarray,
    frame_count: np.ndarray,
    camera_systime: np.ndarray,
    is_valid_camera_time: np.ndarray,
    pause_mid_time: float,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Remove acquisition timing pause non-PTP.

    Parameters
    ----------
    dio_systime : np.ndarray
        Digital I/O system time.
    frame_count : np.ndarray
        Frame count.
    camera_systime : np.ndarray
        Camera system time.
    is_valid_camera_time : np.ndarray
        Boolean array indicating whether the camera time is valid.
    pause_mid_time : float
        Midpoint time of the pause.

    Returns
    -------
    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]
        A tuple containing the following arrays:
        - dio_systime : np.ndarray
            Digital I/O system time after removing the pause.
        - frame_count : np.ndarray
            Frame count after removing the pause.
        - is_valid_camera_time : np.ndarray
            Boolean array indicating whether the camera time is valid after removing the pause.
        - camera_systime : np.ndarray
            Camera system time after removing the pause.
    """
    dio_systime = dio_systime[dio_systime > pause_mid_time]
    frame_count = frame_count[is_valid_camera_time][camera_systime > pause_mid_time]
    is_valid_camera_time[is_valid_camera_time] = camera_systime > pause_mid_time
    camera_systime = camera_systime[camera_systime > pause_mid_time]

    return dio_systime, frame_count, is_valid_camera_time, camera_systime


def correct_timestamps_for_camera_to_mcu_lag(
    frame_count: np.ndarray, camera_systime: np.ndarray, camera_to_mcu_lag: np.ndarray
) -> np.ndarray:
    regression_result = linregress(frame_count, camera_systime - camera_to_mcu_lag)
    corrected_camera_systime = (
        regression_result.intercept + frame_count * regression_result.slope
    )
    corrected_camera_systime /= NANOSECONDS_PER_SECOND

    return corrected_camera_systime


def find_camera_dio_channel(dios):
    raise NotImplementedError


def get_video_timestamps(video_timestamps_filepath: Path) -> np.ndarray:
    """
    Get video timestamps.

    Parameters
    ----------
    video_timestamps_filepath : Path
        Path to the video timestamps file.

    Returns
    -------
    np.ndarray
        An array of video timestamps.
    """
    # Get video timestamps
    video_timestamps = (
        pd.DataFrame(read_trodes_datafile(video_timestamps_filepath)["data"])
        .set_index("PosTimestamp")
        .rename(columns={"frameCount": "HWframeCount"})
    )
    return np.asarray(video_timestamps.HWTimestamp)


def get_position_timestamps(
    position_timestamps_filepath: Path,
    position_tracking_filepath=None | Path,
    mcu_neural_timestamps=None | np.ndarray,
    dios=None,
    ptp_enabled: bool = True,
):
    logger = logging.getLogger("convert")

    # Get video timestamps
    video_timestamps = (
        pd.DataFrame(read_trodes_datafile(position_timestamps_filepath)["data"])
        .set_index("PosTimestamp")
        .rename(columns={"frameCount": "HWframeCount"})
    )

    # On AVT cameras, HWFrame counts wraps to 0 above this value.
    video_timestamps["HWframeCount"] = np.unwrap(
        video_timestamps["HWframeCount"].astype(np.int32),
        period=np.iinfo(np.uint16).max,
    )
    # Keep track of video frames
    video_timestamps["video_frame_ind"] = np.arange(len(video_timestamps))

    # Disconnects manifest as repeats in the trodes time index
    (
        non_repeat_timestamp_labels,
        non_repeat_timestamp_labels_id,
    ) = detect_trodes_time_repeats_or_frame_jumps(
        video_timestamps.index, video_timestamps.HWframeCount
    )
    logger.info(f"\tnon_repeat_timestamp_labels = {non_repeat_timestamp_labels_id}")
    video_timestamps["non_repeat_timestamp_labels"] = non_repeat_timestamp_labels
    video_timestamps = video_timestamps.loc[
        video_timestamps.non_repeat_timestamp_labels > 0
    ]

    # Get position tracking information
    try:
        position_tracking = pd.DataFrame(
            read_trodes_datafile(position_tracking_filepath)["data"]
        ).set_index("time")
        is_repeat_timestamp = detect_repeat_timestamps(position_tracking.index)
        position_tracking = position_tracking.iloc[~is_repeat_timestamp]

        # Match the camera frames to the position tracking
        # Number of video frames can be different from online tracking because
        # online tracking can be started or stopped before video is stopped.
        # Additionally, for offline tracking, frames can be skipped if the
        # frame is labeled as bad.
        video_timestamps = pd.merge(
            video_timestamps,
            position_tracking,
            right_index=True,
            left_index=True,
            how="left",
        )
    except (FileNotFoundError, TypeError):
        pass

    if ptp_enabled:
        ptp_systime = np.asarray(video_timestamps.HWTimestamp)
        # Convert from integer nanoseconds to float seconds
        ptp_timestamps = pd.Index(ptp_systime / NANOSECONDS_PER_SECOND, name="time")
        video_timestamps = video_timestamps.drop(
            columns=["HWframeCount", "HWTimestamp"]
        ).set_index(ptp_timestamps)

        # Ignore positions before the timing pause.
        pause_mid_ind = (
            np.nonzero(
                np.logical_and(
                    np.diff(video_timestamps.index[:100]) > 0.4,
                    np.diff(video_timestamps.index[:100]) < 2.0,
                )
            )[0][0]
            + 1
        )
        original_video_timestamps = video_timestamps.copy()
        video_timestamps = video_timestamps.iloc[pause_mid_ind:]
        logger.info(
            "Camera frame rate estimated from MCU timestamps:"
            f" {1 / np.median(np.diff(video_timestamps.index)):0.1f} frames/s"
        )
        return video_timestamps, original_video_timestamps
    else:
        dio_camera_ticks = find_camera_dio_channel(dios)
        is_valid_tick = np.isin(dio_camera_ticks, mcu_neural_timestamps.index)
        dio_systime = np.asarray(
            mcu_neural_timestamps.loc[dio_camera_ticks[is_valid_tick]]
        )
        # The DIOs and camera frames are initially unaligned. There is a
        # half second pause at the start to allow for alignment.
        pause_mid_time = find_acquisition_timing_pause(dio_systime)

        # Estimate the frame rate from the DIO camera ticks as a sanity check.
        frame_rate_from_dio = get_framerate(dio_systime[dio_systime > pause_mid_time])
        logger.info(
            "Camera frame rate estimated from DIO camera ticks:"
            f" {frame_rate_from_dio:0.1f} frames/s"
        )
        frame_count = np.asarray(video_timestamps.HWframeCount)

        camera_systime, is_valid_camera_time = estimate_camera_time_from_mcu_time(
            video_timestamps, mcu_neural_timestamps
        )
        (
            dio_systime,
            frame_count,
            is_valid_camera_time,
            camera_systime,
        ) = remove_acquisition_timing_pause_non_ptp(
            dio_systime,
            frame_count,
            camera_systime,
            is_valid_camera_time,
            pause_mid_time,
        )
        original_video_timestamps = video_timestamps.copy()
        video_timestamps = video_timestamps.iloc[is_valid_camera_time]

        frame_rate_from_camera_systime = get_framerate(camera_systime)
        logger.info(
            "Camera frame rate estimated from MCU timestamps:"
            f" {frame_rate_from_camera_systime:0.1f} frames/s"
        )

        camera_to_mcu_lag = estimate_camera_to_mcu_lag(
            camera_systime, dio_systime, len(non_repeat_timestamp_labels_id)
        )
        corrected_camera_systime = []
        for id in non_repeat_timestamp_labels_id:
            is_chunk = video_timestamps.non_repeat_timestamp_labels == id
            corrected_camera_systime.append(
                correct_timestamps_for_camera_to_mcu_lag(
                    frame_count[is_chunk],
                    camera_systime[is_chunk],
                    camera_to_mcu_lag,
                )
            )
        corrected_camera_systime = np.concatenate(corrected_camera_systime)
        video_timestamps.iloc[
            is_valid_camera_time
        ].index = corrected_camera_systime.index
        return (
            video_timestamps.set_index(pd.Index(corrected_camera_systime, name="time")),
            original_video_timestamps,
        )


def add_position(
    nwb_file: NWBFile,
    metadata: dict,
    session_df: pd.DataFrame,
    rec_header: ElementTree.ElementTree,
    video_directory: str,
    convert_video: bool = False,
):
    """
    Add position data to an NWBFile.

    Parameters
    ----------
    nwb_file : NWBFile
        The NWBFile to add the position data to.
    metadata : dict
        Metadata about the experiment.
    session_df : pd.DataFrame
        A DataFrame containing information about the session.
    rec_header : ElementTree.ElementTree
        The recording header.
    video_directory : str
        The directory containing the video files.
    convert_video : bool, optional
        Whether to convert the video files to NWB format, by default False.
    """
    logger = logging.getLogger("convert")

    LED_POS_NAMES = [
        [
            "xloc",
            "yloc",
        ],  # led 0
        [
            "xloc2",
            "yloc2",
        ],
    ]  # led 1

    camera_id_to_meters_per_pixel = {
        camera["id"]: camera["meters_per_pixel"] for camera in metadata["cameras"]
    }

    df = []
    for task in metadata["tasks"]:
        df.append(
            pd.DataFrame(
                [(task["camera_id"], epoch) for epoch in task["task_epochs"]],
                columns=["camera_id", "epoch"],
            )
        )

    epoch_to_camera_ids = pd.concat(df).set_index("epoch").sort_index()

    position = Position(name="position")
    ptp_enabled = detect_ptp_from_header(rec_header)

    # Make a processing module for behavior and add to the nwbfile
    if not "behavior" in nwb_file.processing:
        nwb_file.create_processing_module(
            name="behavior", description="Contains all behavior-related data"
        )

    # make processing module for video files
    nwb_file.create_processing_module(
        name="video_files", description="Contains all associated video files data"
    )
    # make a behavioral Event object to hold videos
    video = BehavioralEvents(name="video")

    for epoch in session_df.epoch.unique():
        try:
            position_tracking_filepath = session_df.loc[
                np.logical_and(
                    session_df.epoch == epoch,
                    session_df.file_extension == ".videoPositionTracking",
                )
            ].full_path.to_list()[0]
            # find the matching hw timestamps filepath
            video_index = position_tracking_filepath.split(".")[-2]
            video_hw_df = session_df.loc[
                np.logical_and(
                    session_df.epoch == epoch,
                    session_df.file_extension == ".cameraHWSync",
                )
            ]
            position_timestamps_filepath = video_hw_df[
                [
                    full_path.split(".")[-3] == video_index
                    for full_path in video_hw_df.full_path
                ]
            ].full_path.to_list()[0]

        except IndexError:
            position_tracking_filepath = None

        logger.info(epoch)
        logger.info(f"\tposition_timestamps_filepath: {position_timestamps_filepath}")
        logger.info(f"\tposition_tracking_filepath: {position_tracking_filepath}")

        position_df, original_video_timestamps = get_position_timestamps(
            position_timestamps_filepath,
            position_tracking_filepath,
            ptp_enabled=ptp_enabled,
        )

        # TODO: Doesn't handle multiple cameras currently
        camera_id = epoch_to_camera_ids.loc[epoch].camera_id[0]
        meters_per_pixel = camera_id_to_meters_per_pixel[camera_id]

        if position_tracking_filepath is not None:
            for led_number, valid_keys in enumerate(LED_POS_NAMES):
                key_set = [
                    key for key in position_df.columns.tolist() if key in valid_keys
                ]
                if len(key_set) > 0:
                    position.create_spatial_series(
                        name=f"led_{led_number}_series_{epoch}",
                        description=", ".join(["xloc", "yloc"]),
                        data=np.asarray(position_df[key_set]),
                        conversion=meters_per_pixel,
                        reference_frame="Upper left corner of video frame",
                        timestamps=np.asarray(position_df.index),
                    )
        else:
            position.create_spatial_series(
                name=f"series_{epoch}",
                description=", ".join(["xloc", "yloc"]),
                data=np.asarray([]),
                conversion=meters_per_pixel,
                reference_frame="Upper left corner of video frame",
                timestamps=np.asarray(position_df.index),
            )

        # add the video frame index as a new processing module
        if "position_frame_index" not in nwb_file.processing:
            nwb_file.create_processing_module(
                name="position_frame_index",
                description="stores video frame index for each position timestep",
            )
        # add timeseries for each frame index set (once per series because led's share timestamps)
        nwb_file.processing["position_frame_index"].add(
            TimeSeries(
                name=f"series_{epoch}",
                data=np.asarray(position_df["video_frame_ind"]),
                unit="N/A",
                timestamps=np.asarray(position_df.index),
            )
        )
        # add the video non-repeat timestamp labels as a new processing module
        if "non_repeat_timestamp_labels" not in nwb_file.processing:
            nwb_file.create_processing_module(
                name="non_repeat_timestamp_labels",
                description="stores non_repeat_labels for each position timestep",
            )
        # add timeseries for each non-repeat timestamp labels set (once per series because led's share timestamps)
        nwb_file.processing["non_repeat_timestamp_labels"].add(
            TimeSeries(
                name=f"series_{epoch}",
                data=np.asarray(position_df["non_repeat_timestamp_labels"]),
                unit="N/A",
                timestamps=np.asarray(position_df.index),
            )
        )

        # add the video file data
        # find the video metadata for this epoch
        video_metadata = None
        for vid_ in metadata["associated_video_files"]:
            if vid_["task_epochs"][0] == epoch:
                video_metadata = vid_
                # get the video file path
                video_path = None
                print(video_metadata["name"].rsplit(".", 1)[0])
                for file in session_df.full_path:
                    print(file)
                    if video_metadata["name"].rsplit(".", 1)[0] in file:
                        video_path = file
                        break

                # get timestamps for this video
                video_index = video_path.split(".")[-2]
                video_hw_df = session_df.loc[
                    np.logical_and(
                        session_df.epoch == epoch,
                        session_df.file_extension == ".cameraHWSync",
                    )
                ]
                video_timestamps_filepath = video_hw_df[
                    [
                        full_path.split(".")[-3] == video_index
                        for full_path in video_hw_df.full_path
                    ]
                ].full_path.to_list()[0]
                video_timestamps = get_video_timestamps(video_timestamps_filepath)

                # TODO: copy to video directory if not convert
                if convert_video:
                    video_file_name = convert_h264_to_mp4(video_path)
                else:
                    video_file_name = os.path.join(
                        video_directory, video_metadata["name"]
                    )

                video.add_timeseries(
                    ImageSeries(
                        device=nwb_file.devices[
                            "camera_device " + str(video_metadata["camera_id"])
                        ],
                        name=video_metadata["name"],
                        timestamps=video_timestamps,
                        external_file=[video_file_name],
                        format="external",
                        starting_frame=[0],
                        description="video of animal behavior from epoch",
                    )
                )
        if video_metadata is None:
            raise KeyError(f"Missing video metadata for epoch {epoch}")

    nwb_file.processing["behavior"].add(position)
    nwb_file.processing["video_files"].add(video)


def convert_h264_to_mp4(file: str) -> str:
    """
    Converts h264 file to mp4 file using ffmpeg.

    Parameters
    ----------
    file : str
        The path to the input h264 file.

    Returns
    -------
    str
        The path to the output mp4 file.

    Raises
    ------
    subprocess.CalledProcessError
        If the ffmpeg command fails.

    """
    new_file_name = file.replace(".h264", ".mp4")
    logger = logging.getLogger("convert")
    if os.path.exists(new_file_name):
        return new_file_name
    try:
        # Construct the ffmpeg command
        subprocess.run(f"ffmpeg -i {file} {new_file_name}", shell=True)
        logger.info(
            f"Video conversion completed. {file} has been converted to {new_file_name}"
        )
        return new_file_name
    except subprocess.CalledProcessError as e:
        logger.error(
            f"Video conversion FAILED. {file} has NOT been converted to {new_file_name}"
        )
        raise e
